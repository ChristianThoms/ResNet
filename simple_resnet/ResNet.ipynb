{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xl0g3Bnaln6-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def to_one_hot(y, no_labels):\n",
        "    arr = np.squeeze(np.eye(no_labels)[y])\n",
        "    return arr\n",
        "\n",
        "n_classes = 10\n",
        "cifar10 = tf.keras.datasets.cifar10.load_data()\n",
        "cifar10_train, cifar10_test = cifar10[0], cifar10[1]\n",
        "\n",
        "# use only first 25000 images (full dataset 50k)\n",
        "train_X = cifar10_train[0][:25000]\n",
        "cifar10_train_y = cifar10_train[1][:25000]\n",
        "\n",
        "test_X = cifar10_test[0][:25000]\n",
        "cifar10_test_y = cifar10_test[1][:25000]\n",
        "\n",
        "train_Y = to_one_hot(cifar10_train_y,n_classes)\n",
        "test_Y  = to_one_hot(cifar10_test_y, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SIgdrV4guNU_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# resunit from first paper, conv -> batch -> relu -> conv batch -> shortcut addition -> relu\n",
        "def resUnit(input_layer,i):\n",
        "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
        "        part1 = slim.conv2d(input_layer,64,[3,3],activation_fn=None)\n",
        "        part2 = slim.batch_norm(part1,activation_fn=None)\n",
        "        part3 = tf.nn.relu(part2)\n",
        "        part4 = slim.conv2d(part3,64,[3,3],activation_fn=None)\n",
        "        part5 = slim.batch_norm(part4,activation_fn=None)\n",
        "        shortcut = part5 + input_layer # remove input_layer here for 'plain' network\n",
        "        output = tf.nn.relu(shortcut)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h17GVDkPGlMI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# resnet from follow up paper, batch -> relu -> conv -> batch -> relu -> conv -> shortcut addition\n",
        "def resUnit2(input_layer,i):\n",
        "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
        "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
        "        part2 = tf.nn.relu(part1)\n",
        "        part3 = slim.conv2d(part2,64,[3,3],activation_fn=None)\n",
        "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
        "        part5 = tf.nn.relu(part4)\n",
        "        part6 = slim.conv2d(part5,64,[3,3],activation_fn=None)\n",
        "        output = part6 + input_layer\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKVmdsVf5Gk4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this is not working right now\n",
        "# tried to replace conv with dense layer\n",
        "def denseResNet(input_layer, i):\n",
        "    shortcut = input_layer\n",
        "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
        "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
        "        part2 = tf.nn.relu(part1)\n",
        "        #part3 = slim.conv2d(part2,64,[3,3],activation_fn=None)\n",
        "        part3 = tf.layers.dense(part2, part2.shape[1], activation=None)\n",
        "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
        "        part5 = tf.nn.relu(part4)\n",
        "        part6 = tf.layers.dense(part5, part5.shape[1], activation=None)\n",
        "        #part6 = slim.conv2d(part5,64,[3,3],activation_fn=None)\n",
        "        output =  part6 + shortcut\n",
        "        print(input_layer.shape, part6.shape)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oo-crPVSl2Sg",
        "colab_type": "code",
        "outputId": "d5e96bd4-c56b-413f-c7c3-c4d5e7dae75e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.slim as slim\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "total_layers = 1 # not used atm, Specify how deep we want our network\n",
        "units_between_stride = 2 # how many resUnits between a stride\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "# CIFAR10 data input (img shape: 32*32*3)\n",
        "n_input = 32\n",
        "n_depth = 3\n",
        "\n",
        "# input layer\n",
        "x = tf.placeholder(shape=[None,n_input,n_input,n_depth],dtype=tf.float32,name='input')\n",
        "label_layer = tf.placeholder(shape=[None],dtype=tf.int32)\n",
        "y = slim.layers.one_hot_encoding(label_layer, n_classes)\n",
        "\n",
        "\n",
        "layer1 = slim.conv2d(x,64,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(0))\n",
        "\n",
        "# the following 2 lines are neccessary if you want to use denseResNet() in the next loop\n",
        "# in this case, the above conv2d layer can be removed\n",
        "#layer1 = slim.layers.flatten(x)\n",
        "#layer1 = tf.layers.dense(layer1, 512, activation=tf.nn.relu)\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(units_between_stride):\n",
        "        layer1 = resUnit2(layer1, j + (i*units_between_stride))\n",
        "    layer1 = slim.conv2d(layer1,64,[3,3],stride=[2,2],normalizer_fn=slim.batch_norm, scope='conv_s_'+str(i))\n",
        "\n",
        "#top = slim.conv2d(layer1,10,[3,3],normalizer_fn=slim.batch_norm,activation_fn=None,scope='conv_top')\n",
        "\n",
        "layer1 = slim.layers.flatten(layer1)\n",
        "# get 10 outputs for 10 classes\n",
        "top = tf.layers.dense(layer1, n_classes, activation=tf.nn.relu)\n",
        "\n",
        "output = slim.layers.softmax(top)\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(output) + 1e-10, reduction_indices=[1]))\n",
        "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "update = trainer.minimize(cost)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
        "#calculate accuracy across all the given images and average them out. \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "### unused, ignore\n",
        "def _log_gradients(num_layers):\n",
        "    gr = tf.get_default_graph()\n",
        "    for i in range(num_layers):\n",
        "        weight = gr.get_tensor_by_name('conv_{}/weights:0'.format(i + 1))\n",
        "        grad = tf.gradients(cost, weight)[0]\n",
        "        mean = tf.reduce_mean(tf.abs(grad))\n",
        "        print('mean_{}'.format(i + 1), mean)\n",
        "        print('histogram_{}'.format(i + 1), str(grad))\n",
        "        print('hist_weights_{}'.format(i + 1), str(grad))\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KLbPPX5nl7Fx",
        "colab_type": "code",
        "outputId": "3fc67583-4921-42f9-a075-396c559671c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1109
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "training_iters = 10\n",
        "with tf.Session() as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(init)\n",
        "    # restore saved weights\n",
        "    #saver.restore(sess, \"./plain2-20\")\n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "    train_accuracy = []\n",
        "    test_accuracy = []\n",
        "    #summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
        "    \n",
        "    for i in range(training_iters):\n",
        "        iter_loss = []\n",
        "        iter_accuracy = []\n",
        "        \n",
        "        # training\n",
        "        for batch in range(len(train_X)//batch_size):\n",
        "            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
        "            batch_y = train_Y[batch*batch_size:min((batch+1)*batch_size,len(train_Y))]\n",
        "            # Run optimization op (backprop).\n",
        "            # Calculate batch loss and accuracy\n",
        "            opt = sess.run(update, feed_dict={x: batch_x, y: batch_y})\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "            iter_loss.append(loss)\n",
        "            iter_accuracy.append(acc)\n",
        "\n",
        "        train_loss.append(sum(iter_loss)/(len(train_X)//batch_size))\n",
        "        train_accuracy.append(sum(iter_accuracy)/(len(train_X)//batch_size))\n",
        "        print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
        "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                      \"{:.5f} \".format(acc), end='')\n",
        "\n",
        "        # Calculate accuracy for all test images\n",
        "        iter_loss = []\n",
        "        iter_accuracy = []\n",
        "        \n",
        "        # testing\n",
        "        for batch in range(len(test_X)//batch_size):\n",
        "\n",
        "            batch_x = test_X[batch*batch_size:min((batch+1)*batch_size,len(test_X))]\n",
        "            batch_y = test_Y[batch*batch_size:min((batch+1)*batch_size,len(test_Y))]\n",
        "            valid_loss, test_acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
        "            iter_loss.append(valid_loss)\n",
        "            iter_accuracy.append(test_acc)\n",
        "\n",
        "        \n",
        "        test_loss.append(sum(iter_loss)/(len(test_X)//batch_size))\n",
        "        test_accuracy.append(sum(iter_accuracy)/(len(test_X)//batch_size))\n",
        "        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_accuracy[i]))\n",
        "        \n",
        "    # save weights\n",
        "    save_path = saver.save(sess, './orig2_resNet',global_step=training_iters)\n",
        "    \n",
        "    end = time.time()\n",
        "    print(\"duration:\", end-start)\n",
        "    \n",
        "    #summary_writer.close()\n",
        "    \n",
        "    #tvars = tf.trainable_variables()\n",
        "    #tvars_vals = sess.run(tvars)\n",
        "\n",
        "    #for var, val in zip(tvars, tvars_vals):\n",
        "        #print(var.name, val)  # Prints the name of the variable alongside its value."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0, Loss= 2.087293, Training Accuracy= 0.21875 Testing Accuracy: 0.27214\n",
            "Iter 1, Loss= 1.824421, Training Accuracy= 0.35938 Testing Accuracy: 0.35777\n",
            "Iter 2, Loss= 1.608509, Training Accuracy= 0.48438 Testing Accuracy: 0.41647\n",
            "Iter 3, Loss= 1.537956, Training Accuracy= 0.46875 Testing Accuracy: 0.44401\n",
            "Iter 4, Loss= 1.441008, Training Accuracy= 0.51562 Testing Accuracy: 0.45713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-e4dcb9871173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Run optimization op (backprop).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rUiNQYV0aonC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}